[
  {
    "objectID": "posts/Blog_Post_1/index.html",
    "href": "posts/Blog_Post_1/index.html",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "In the world of modeling, statistics, and machine learning, precision and recall are crucial metrics. However it can be confusing to understand the difference between these two concepts. In this blog post, I’m going to walk you through an interactive example using R code. To keep things relatively simple, our model of choice will be logistic regression. For our dataset, we’ll be using the Breast Cancer Wisconsin Dataset. It is a diagnostic dataset that contains 10 features of a given cell, such as thickness, cell size, shape, etc. There is a “class” variable that shows whether the diagnosis for this cell was “malignant” or “benign.”\nWe will be using logistic regression to help us determine whether a given patient has a “malignant” or “benign” tumor. This dataset is well-suited to using logistic regression because there are two outcome variables. We’ll then walk through an example of a model that prioritizes high precision and another model that optimizes high recall.\nLet’s get started!\n\n\n\nThe first thing we need to understand before precision and recall are our classification outcomes. These outcomes can be categorized into four types based on the accuracy of the predictions in relation to the actual data:\n\nTrue Positives (TP): These are the cases where the model correctly predicts the positive class. For example, the model correctly identifies a patient as having a malignant tumor.\nFalse Positives (FP): These occur when the model incorrectly predicts the positive class. For example, the model mistakenly identifies a patient with a benign tumor as having a malignant tumor.\nTrue Negatives (TN): These are the cases where the model correctly predicts the negative class. For example, the model correctly identifies a patient as not having a malignant tumor.\nFalse Negatives (FN): These occur when the model fails to predict the positive class. For example, the model mistakenly identifies a patient with a malignant tumor as having a benign tumor.\n\n\n\n\nPrecision and Recall are essential metrics to understand, especially in cases where identifying true positives and avoiding false positives is crucial.\n\nPrecision measures the accuracy of positive predictions made by the model. It is the ratio of true positives to the sum of true and call positives. When we have high precision, this indicates a low rate of false positives.\nRecall measures the ability of the model to find all relevant cases within a dataset. It is the ratio of true positives to the sum of true positives and false negatives. High recall indicates that the model identified most of the actual positives.\n\nAnother critical point is that Precision and Recall are often inversely related—meaning that improving one can sometimes lead to a reduction of performance in the other. So how do we now which metric we want to optimize? It all depends on our application.\nIn this post, we’re going to think about this question using the breast cancer dataset. In this blog post, we’re going to walk through some interactive examples to help us figure out in this application which we should optimize—precision or recall.\n\n\n\nSince this will be an interactive tutorial using R, we’re going to start by loading in the appropriate R packages.\ntidyverse is a collection of R packages useful for statistics and data science. It contains other commonly used packages like ggplot2 for data visualization, dplyr for data manipulation, and readr for reading data inside of it.\nbroom helps with displaying our data into tidy data frames\nmlbench is the library we’ll use to load in our dataset\nmodelsummary provides customizeable summary tables for statistical models in R\nglmnet is what we’ll use to implement generalized linear models. We’re going to use this instead of glm because it is a bit more robust with regularization to help prevent overfitting.\ncaret caret stands for Classification And REgression Training and it will help us with model training\nIf you don’t have any of these packages installed, then uncomment the install.packages line and add in the packages you need to follow along.\n\n# install.packages(\"mlbench\")\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(mlbench)\nlibrary(glmnet)\nlibrary(caret)\n\n\n\n\nNow we’ll load in the dataset using the mlbench library. In this code block, we’re also going to convert the Class column of the breast_cancer dataset from a categorical string format into a binary numeric format. Specifically, it changes the value “malignant” to 1 and all other values (which in this case is “benign”) to 0. This preprocessing is a necessary step for using our logistic regression model in the next step. Finally, we’ll display a small portion of the breast_cancer dataset using the head function.\n\n# Load in the dataset\ndata(\"BreastCancer\", package = \"mlbench\")\n\n# Assign to the variable breast_cancer\nbreast_cancer &lt;- BreastCancer\n\n# Convert the Class column from a categorical string format to binary numeric format  \nbreast_cancer$Class &lt;- as.factor(ifelse(breast_cancer$Class == \"malignant\", 1, 0))\n\n# Display a few rows of the dataset\nhead(breast_cancer) \n\n       Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size\n1 1000025            5         1          1             1            2\n2 1002945            5         4          4             5            7\n3 1015425            3         1          1             1            2\n4 1016277            6         8          8             1            3\n5 1017023            4         1          1             3            2\n6 1017122            8        10         10             8            7\n  Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses Class\n1           1           3               1       1     0\n2          10           3               2       1     0\n3           2           3               1       1     0\n4           4           3               7       1     0\n5           1           3               1       1     0\n6          10           9               7       1     1\n\n\nWe can see from the head(breast_cancer) output above that things seem to be looking in order. We can see that our Class column has 0s and 1s as expected, and we can also see those other features of the dataset we discussed before such as thickness, cell size, cell shape, etc. represented in the columns. It looks like we’re ready to move onto the next step!\n\n\n\nThe next step to understanding precision and recall is to separate our dataset into train and test splits. Its standard practice in machine learning to use an 80%/20% split, meaning 80% of our dataset is reserved for training, while 20% is used for testing the model performance. Its important to reserve a portion of data that the model has not seen before for evaluating the performance later on.\nWe’ll also do something called setting the seed (in this case we’re using 123). Basically, the reason we do this is because it turns out that the problem of true, unpredictable randomness is very difficult in computing. The “random” shuffles we produce for splitting 80% of the rows into training and 20% into testing is actually deterministic based on this seed value that we set. Its actually a good thing though for reproducability of our experiments. If we had this in a paper and as long as we document what seed value we use, our collaborators should be able to reproduce the same test and train splits as we did.\nIn order to implement this testing and training split, we’ll use a function called createDataPartition which comes from the caret package we installed earlier. Then we’ll assign train_data to the 80% partition we created, and then test_data will use the remaining 20% of the rows from the breast_cancer dataset.\n\n# Setting the seed for reproducability \nset.seed(123)\n\n# Splitting the dataset \n# p=0.80 means that 80% of the data will be used for trianing \ntraining_rows &lt;- createDataPartition(breast_cancer$Class, p = 0.8, list = FALSE)\n\n# Creating the training data set from the 80% split above \ntrain_data &lt;- breast_cancer[training_rows, ]\n\n# Using the opposite rows (20% split) for the testing data \ntest_data &lt;- breast_cancer[-training_rows, ]\n\n\n\n\nNow that we have our testing and training data all set up, the next step is to create our actual logistic regression model in R.\nThe first thing that we’ll do here to create a model matrix. This is necessary for the glmnet function that we’re using, because it expects the data in a certain format. This function will ensure that all our data is numeric and in the proper format before passing it into the logistic regression function.\nOur train set is separated into x_train and y_train—all that this means is that the data in x_train are the predictor variables, whereas y_train is our class variable (AKA, malignant or benign).\nNext, we’re going to call the cv.glmnet function, which fits a logistic regression model using cross-validation to optimize the lambda parameter, which controls the strength of the regularization. The family = \"binomial\" argument specifies that it is a logistic regression for binary classification.\nThen, we’ll generate a plot of the cross-validation results from cv_model.\n\n# Prepare model matrix for the predictors \nx_train &lt;- model.matrix(Class ~ Cl.thickness + Cell.size + Cell.shape, train_data)[,-1]\n\n# Extract response variable \ny_train &lt;- train_data$Class\n\n# Fit a regularized logistic regression model \ncv_model &lt;- cv.glmnet(x_train, y_train, family = \"binomial\")\n\n# Plotting the cross-validation results \nplot(cv_model)\n\n\n\n\nFinally, we’ll add in some code to prepare the test data and make predictions using our trained logistic regression model.\n\n# Creating the Model Matrix for Test Data\nx_test &lt;- model.matrix(Class ~ Cl.thickness + Cell.size + Cell.shape, test_data)[,-1]\n# Making Predictions\npredictions &lt;- predict(cv_model, newx = x_test, s = \"lambda.min\", type = \"response\")\n\n\n\n\nNow that we have our logistic regression model, we’re going to use it to show how adjusting the threshold can impact precision and recall.\n\n\nIn logistic regression models, the output is a probability that indicates the model’s confidence level in predicting the positive class. This probability ranges between 0 and 1.\nThe threshold is a critical parameter that you set to decide when to classify a prediction as positive (1) or negative (0). For example, a common default threshold is 0.5. If the model’s predicted probability for a positive class is greater than 0.5, the prediction is classified as positive; otherwise, it is negative. Adjusting this threshold affects the balance between sensitivity (recall) and specificity, impacting the counts of true positives, false positives, true negatives, and false negatives in your results. This adjustment allows you to tailor model performance to specific needs, depending on which outcomes (e.g., false positives vs. false negatives) carry a higher cost in your application context.\n\n\n\nIn this example, we will use a high confidence threshold to demonstrate how this affects the precision and recall. In this example, we’re going to set our threshold to 0.9. Higher thresholds tend to produce models with higher precision (fewer false positives) but potentially lower recall (more false negatives).\n\n# Assuming 'predictions' contains model predicted probabilities\npredicted_high_precision &lt;- ifelse(predictions &gt; 0.9, 1, 0)  # High threshold\nconf_high_precision &lt;- table(Predicted = predicted_high_precision, Actual = test_data$Class)\nprecision_high_precision &lt;- prop.table(conf_high_precision, 1)[2, 2]\nrecall_high_precision &lt;- prop.table(conf_high_precision, 2)[2, 2]\n\nprint(paste(\"Precision:\", precision_high_precision))\n\n[1] \"Precision: 1\"\n\nprint(paste(\"Recall:\", recall_high_precision))\n\n[1] \"Recall: 0.645833333333333\"\n\n\nTo examine the output a bit further, lets look at the confusion matrix. A confusion matrix is a tool used to describe the performance of a classification model, and it will organize the output into predicted and actual classifications: * Bottom-right: True positives - Correct positive predictions. * Bottom-left: False positives - Incorrectly predicted positives. * Top-right: False negatives - Incorrectly predicted negatives. * Top-left: True negatives - Correct negative predictions.\n\nconf_mat &lt;- table(Predicted = predicted_high_precision, Actual = test_data$Class)\nconf_mat\n\n         Actual\nPredicted  0  1\n        0 91 17\n        1  0 31\n\n\nIn this case, we can calculate our precision and recall manually by doing the following:\n\n# Precision = (TP)/(TP + FP)\nprecision = (31)/(31+17)\nprecision\n\n[1] 0.6458333\n\n# Recall = (TP)/(TP+FN)\nrecall = 31/(31+0)\nrecall\n\n[1] 1\n\n\nAnd this matches our above output! The interpretation of a model with high precision and low recall is that this model will miss many examples of the “malignant” tumor, but amongst all the ones it does classify as a malignant tumor it is correct.\nThis would be the equivalent of a doctor who misses about 35% of cases with a malignant tumor. Yikes! Let’s see if we can do better than that.\n\n\n\nIn this example, we’re going to set the threshold low and see how this affects precision and recall.\n\npredicted_high_recall &lt;- ifelse(predictions &gt; 0.1, 1, 0)  # Low threshold\nconf_high_recall &lt;- table(Predicted = predicted_high_recall, Actual = test_data$Class)\nprecision_high_recall &lt;- prop.table(conf_high_recall, 1)[2, 2]\nrecall_high_recall &lt;- prop.table(conf_high_recall, 2)[2, 2]\n\nprint(paste(\"Precision:\", precision_high_recall))\n\n[1] \"Precision: 0.774193548387097\"\n\nprint(paste(\"Recall:\", recall_high_recall))\n\n[1] \"Recall: 1\"\n\n\nWe can see here that precision is about 76% and recall is at 100%. So this means that amongst all the patients they’ve ever had, they’ve never missed any cases of malignant tumors. However for about 24% of cases, the doctor will tell the patient they have cancer and they actually have a benign tumor. Not ideal, but certainly better than the previous case where 35% of patients with cancer would go untreated!\n\n\n\n\nIn conclusion, we can see how the choice between prioritizing precision or recall depends on the context of the problem. In this example in the field of medical diagnostics, missing a positive diagnosis (low recall) is probably more harmful than a false positive (low precision). We’ve seen how given the exact same model, we can show how the otuput can significantly change by tuning this confidence threshold.\nHopefully you can take this knowledge and apply it in your own models in the future!"
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#introduction",
    "href": "posts/Blog_Post_1/index.html#introduction",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "In the world of modeling, statistics, and machine learning, precision and recall are crucial metrics. However it can be confusing to understand the difference between these two concepts. In this blog post, I’m going to walk you through an interactive example using R code. To keep things relatively simple, our model of choice will be logistic regression. For our dataset, we’ll be using the Breast Cancer Wisconsin Dataset. It is a diagnostic dataset that contains 10 features of a given cell, such as thickness, cell size, shape, etc. There is a “class” variable that shows whether the diagnosis for this cell was “malignant” or “benign.”\nWe will be using logistic regression to help us determine whether a given patient has a “malignant” or “benign” tumor. This dataset is well-suited to using logistic regression because there are two outcome variables. We’ll then walk through an example of a model that prioritizes high precision and another model that optimizes high recall.\nLet’s get started!"
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#understanding-classification-outcomes-true-positives-false-positives-true-negatives-and-false-negatives",
    "href": "posts/Blog_Post_1/index.html#understanding-classification-outcomes-true-positives-false-positives-true-negatives-and-false-negatives",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "The first thing we need to understand before precision and recall are our classification outcomes. These outcomes can be categorized into four types based on the accuracy of the predictions in relation to the actual data:\n\nTrue Positives (TP): These are the cases where the model correctly predicts the positive class. For example, the model correctly identifies a patient as having a malignant tumor.\nFalse Positives (FP): These occur when the model incorrectly predicts the positive class. For example, the model mistakenly identifies a patient with a benign tumor as having a malignant tumor.\nTrue Negatives (TN): These are the cases where the model correctly predicts the negative class. For example, the model correctly identifies a patient as not having a malignant tumor.\nFalse Negatives (FN): These occur when the model fails to predict the positive class. For example, the model mistakenly identifies a patient with a malignant tumor as having a benign tumor."
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#what-are-precision-and-recall",
    "href": "posts/Blog_Post_1/index.html#what-are-precision-and-recall",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "Precision and Recall are essential metrics to understand, especially in cases where identifying true positives and avoiding false positives is crucial.\n\nPrecision measures the accuracy of positive predictions made by the model. It is the ratio of true positives to the sum of true and call positives. When we have high precision, this indicates a low rate of false positives.\nRecall measures the ability of the model to find all relevant cases within a dataset. It is the ratio of true positives to the sum of true positives and false negatives. High recall indicates that the model identified most of the actual positives.\n\nAnother critical point is that Precision and Recall are often inversely related—meaning that improving one can sometimes lead to a reduction of performance in the other. So how do we now which metric we want to optimize? It all depends on our application.\nIn this post, we’re going to think about this question using the breast cancer dataset. In this blog post, we’re going to walk through some interactive examples to help us figure out in this application which we should optimize—precision or recall."
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#loading-the-r-packages",
    "href": "posts/Blog_Post_1/index.html#loading-the-r-packages",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "Since this will be an interactive tutorial using R, we’re going to start by loading in the appropriate R packages.\ntidyverse is a collection of R packages useful for statistics and data science. It contains other commonly used packages like ggplot2 for data visualization, dplyr for data manipulation, and readr for reading data inside of it.\nbroom helps with displaying our data into tidy data frames\nmlbench is the library we’ll use to load in our dataset\nmodelsummary provides customizeable summary tables for statistical models in R\nglmnet is what we’ll use to implement generalized linear models. We’re going to use this instead of glm because it is a bit more robust with regularization to help prevent overfitting.\ncaret caret stands for Classification And REgression Training and it will help us with model training\nIf you don’t have any of these packages installed, then uncomment the install.packages line and add in the packages you need to follow along.\n\n# install.packages(\"mlbench\")\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(mlbench)\nlibrary(glmnet)\nlibrary(caret)"
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#load-the-dataset",
    "href": "posts/Blog_Post_1/index.html#load-the-dataset",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "Now we’ll load in the dataset using the mlbench library. In this code block, we’re also going to convert the Class column of the breast_cancer dataset from a categorical string format into a binary numeric format. Specifically, it changes the value “malignant” to 1 and all other values (which in this case is “benign”) to 0. This preprocessing is a necessary step for using our logistic regression model in the next step. Finally, we’ll display a small portion of the breast_cancer dataset using the head function.\n\n# Load in the dataset\ndata(\"BreastCancer\", package = \"mlbench\")\n\n# Assign to the variable breast_cancer\nbreast_cancer &lt;- BreastCancer\n\n# Convert the Class column from a categorical string format to binary numeric format  \nbreast_cancer$Class &lt;- as.factor(ifelse(breast_cancer$Class == \"malignant\", 1, 0))\n\n# Display a few rows of the dataset\nhead(breast_cancer) \n\n       Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size\n1 1000025            5         1          1             1            2\n2 1002945            5         4          4             5            7\n3 1015425            3         1          1             1            2\n4 1016277            6         8          8             1            3\n5 1017023            4         1          1             3            2\n6 1017122            8        10         10             8            7\n  Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses Class\n1           1           3               1       1     0\n2          10           3               2       1     0\n3           2           3               1       1     0\n4           4           3               7       1     0\n5           1           3               1       1     0\n6          10           9               7       1     1\n\n\nWe can see from the head(breast_cancer) output above that things seem to be looking in order. We can see that our Class column has 0s and 1s as expected, and we can also see those other features of the dataset we discussed before such as thickness, cell size, cell shape, etc. represented in the columns. It looks like we’re ready to move onto the next step!"
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#splitting-the-dataset-into-test-and-train-sets",
    "href": "posts/Blog_Post_1/index.html#splitting-the-dataset-into-test-and-train-sets",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "The next step to understanding precision and recall is to separate our dataset into train and test splits. Its standard practice in machine learning to use an 80%/20% split, meaning 80% of our dataset is reserved for training, while 20% is used for testing the model performance. Its important to reserve a portion of data that the model has not seen before for evaluating the performance later on.\nWe’ll also do something called setting the seed (in this case we’re using 123). Basically, the reason we do this is because it turns out that the problem of true, unpredictable randomness is very difficult in computing. The “random” shuffles we produce for splitting 80% of the rows into training and 20% into testing is actually deterministic based on this seed value that we set. Its actually a good thing though for reproducability of our experiments. If we had this in a paper and as long as we document what seed value we use, our collaborators should be able to reproduce the same test and train splits as we did.\nIn order to implement this testing and training split, we’ll use a function called createDataPartition which comes from the caret package we installed earlier. Then we’ll assign train_data to the 80% partition we created, and then test_data will use the remaining 20% of the rows from the breast_cancer dataset.\n\n# Setting the seed for reproducability \nset.seed(123)\n\n# Splitting the dataset \n# p=0.80 means that 80% of the data will be used for trianing \ntraining_rows &lt;- createDataPartition(breast_cancer$Class, p = 0.8, list = FALSE)\n\n# Creating the training data set from the 80% split above \ntrain_data &lt;- breast_cancer[training_rows, ]\n\n# Using the opposite rows (20% split) for the testing data \ntest_data &lt;- breast_cancer[-training_rows, ]"
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#regularizing-logistic-regression-models-in-r",
    "href": "posts/Blog_Post_1/index.html#regularizing-logistic-regression-models-in-r",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "Now that we have our testing and training data all set up, the next step is to create our actual logistic regression model in R.\nThe first thing that we’ll do here to create a model matrix. This is necessary for the glmnet function that we’re using, because it expects the data in a certain format. This function will ensure that all our data is numeric and in the proper format before passing it into the logistic regression function.\nOur train set is separated into x_train and y_train—all that this means is that the data in x_train are the predictor variables, whereas y_train is our class variable (AKA, malignant or benign).\nNext, we’re going to call the cv.glmnet function, which fits a logistic regression model using cross-validation to optimize the lambda parameter, which controls the strength of the regularization. The family = \"binomial\" argument specifies that it is a logistic regression for binary classification.\nThen, we’ll generate a plot of the cross-validation results from cv_model.\n\n# Prepare model matrix for the predictors \nx_train &lt;- model.matrix(Class ~ Cl.thickness + Cell.size + Cell.shape, train_data)[,-1]\n\n# Extract response variable \ny_train &lt;- train_data$Class\n\n# Fit a regularized logistic regression model \ncv_model &lt;- cv.glmnet(x_train, y_train, family = \"binomial\")\n\n# Plotting the cross-validation results \nplot(cv_model)\n\n\n\n\nFinally, we’ll add in some code to prepare the test data and make predictions using our trained logistic regression model.\n\n# Creating the Model Matrix for Test Data\nx_test &lt;- model.matrix(Class ~ Cl.thickness + Cell.size + Cell.shape, test_data)[,-1]\n# Making Predictions\npredictions &lt;- predict(cv_model, newx = x_test, s = \"lambda.min\", type = \"response\")"
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#demonstrating-the-trade-off-with-r-code",
    "href": "posts/Blog_Post_1/index.html#demonstrating-the-trade-off-with-r-code",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "Now that we have our logistic regression model, we’re going to use it to show how adjusting the threshold can impact precision and recall.\n\n\nIn logistic regression models, the output is a probability that indicates the model’s confidence level in predicting the positive class. This probability ranges between 0 and 1.\nThe threshold is a critical parameter that you set to decide when to classify a prediction as positive (1) or negative (0). For example, a common default threshold is 0.5. If the model’s predicted probability for a positive class is greater than 0.5, the prediction is classified as positive; otherwise, it is negative. Adjusting this threshold affects the balance between sensitivity (recall) and specificity, impacting the counts of true positives, false positives, true negatives, and false negatives in your results. This adjustment allows you to tailor model performance to specific needs, depending on which outcomes (e.g., false positives vs. false negatives) carry a higher cost in your application context.\n\n\n\nIn this example, we will use a high confidence threshold to demonstrate how this affects the precision and recall. In this example, we’re going to set our threshold to 0.9. Higher thresholds tend to produce models with higher precision (fewer false positives) but potentially lower recall (more false negatives).\n\n# Assuming 'predictions' contains model predicted probabilities\npredicted_high_precision &lt;- ifelse(predictions &gt; 0.9, 1, 0)  # High threshold\nconf_high_precision &lt;- table(Predicted = predicted_high_precision, Actual = test_data$Class)\nprecision_high_precision &lt;- prop.table(conf_high_precision, 1)[2, 2]\nrecall_high_precision &lt;- prop.table(conf_high_precision, 2)[2, 2]\n\nprint(paste(\"Precision:\", precision_high_precision))\n\n[1] \"Precision: 1\"\n\nprint(paste(\"Recall:\", recall_high_precision))\n\n[1] \"Recall: 0.645833333333333\"\n\n\nTo examine the output a bit further, lets look at the confusion matrix. A confusion matrix is a tool used to describe the performance of a classification model, and it will organize the output into predicted and actual classifications: * Bottom-right: True positives - Correct positive predictions. * Bottom-left: False positives - Incorrectly predicted positives. * Top-right: False negatives - Incorrectly predicted negatives. * Top-left: True negatives - Correct negative predictions.\n\nconf_mat &lt;- table(Predicted = predicted_high_precision, Actual = test_data$Class)\nconf_mat\n\n         Actual\nPredicted  0  1\n        0 91 17\n        1  0 31\n\n\nIn this case, we can calculate our precision and recall manually by doing the following:\n\n# Precision = (TP)/(TP + FP)\nprecision = (31)/(31+17)\nprecision\n\n[1] 0.6458333\n\n# Recall = (TP)/(TP+FN)\nrecall = 31/(31+0)\nrecall\n\n[1] 1\n\n\nAnd this matches our above output! The interpretation of a model with high precision and low recall is that this model will miss many examples of the “malignant” tumor, but amongst all the ones it does classify as a malignant tumor it is correct.\nThis would be the equivalent of a doctor who misses about 35% of cases with a malignant tumor. Yikes! Let’s see if we can do better than that.\n\n\n\nIn this example, we’re going to set the threshold low and see how this affects precision and recall.\n\npredicted_high_recall &lt;- ifelse(predictions &gt; 0.1, 1, 0)  # Low threshold\nconf_high_recall &lt;- table(Predicted = predicted_high_recall, Actual = test_data$Class)\nprecision_high_recall &lt;- prop.table(conf_high_recall, 1)[2, 2]\nrecall_high_recall &lt;- prop.table(conf_high_recall, 2)[2, 2]\n\nprint(paste(\"Precision:\", precision_high_recall))\n\n[1] \"Precision: 0.774193548387097\"\n\nprint(paste(\"Recall:\", recall_high_recall))\n\n[1] \"Recall: 1\"\n\n\nWe can see here that precision is about 76% and recall is at 100%. So this means that amongst all the patients they’ve ever had, they’ve never missed any cases of malignant tumors. However for about 24% of cases, the doctor will tell the patient they have cancer and they actually have a benign tumor. Not ideal, but certainly better than the previous case where 35% of patients with cancer would go untreated!"
  },
  {
    "objectID": "posts/Blog_Post_1/index.html#conclusion",
    "href": "posts/Blog_Post_1/index.html#conclusion",
    "title": "PSY 504 Blog Post",
    "section": "",
    "text": "In conclusion, we can see how the choice between prioritizing precision or recall depends on the context of the problem. In this example in the field of medical diagnostics, missing a positive diagnosis (low recall) is probably more harmful than a false positive (low precision). We’ve seen how given the exact same model, we can show how the otuput can significantly change by tuning this confidence threshold.\nHopefully you can take this knowledge and apply it in your own models in the future!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]